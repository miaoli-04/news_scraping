{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "749cdb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "def make_link_absolute(rel_url, current_url):\n",
    "    \"\"\"\n",
    "    Given a relative URL like \"/abc/def\" or \"?page=2\"\n",
    "    and a complete URL like \"https://example.com/1/2/3\" this function will\n",
    "    combine the two yielding a URL like \"https://example.com/abc/def\"\n",
    "\n",
    "    Parameters:\n",
    "        * rel_url:      a URL or fragment\n",
    "        * current_url:  a complete URL used to make the request that contained a link to rel_url\n",
    "\n",
    "    Returns:\n",
    "        A full URL with protocol & domain that refers to rel_url.\n",
    "    \"\"\"\n",
    "    url = urlparse(current_url)\n",
    "    if rel_url.startswith(\"/\"):\n",
    "        return f\"{url.scheme}://{url.netloc}{rel_url}\"\n",
    "    elif rel_url.startswith(\"?\"):\n",
    "        return f\"{url.scheme}://{url.netloc}{url.path}{rel_url}\"\n",
    "    else:\n",
    "        return rel_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b239a913",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import workbook\n",
    "import requests\n",
    "import lxml.html\n",
    "import openpyxl\n",
    "#the document to write in \n",
    "path = \"/Users/miaoli/Desktop/scraper.xlsx\"\n",
    "\n",
    "workbook = openpyxl.load_workbook(path)\n",
    "\n",
    "sheet = workbook.active\n",
    "sheet.title = \"news\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634edf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_napp(end_date = '2032-01-01'):\n",
    "    '''\n",
    "    scraping NAPP podcast after a certain date\n",
    "    input:\n",
    "        end_data(string): in format like \"2010-01-01\")\n",
    "        if no input: scrape the entire website\n",
    "    '''\n",
    "    import datetime\n",
    "    \n",
    "    end_date = datetime.datetime.strptime(end_date, '%Y-%m-%d')\n",
    "    value = []\n",
    "    url = \"https://effectivegov.uchicago.edu/initiatives/podcast\"\n",
    "    response = requests.get(url)\n",
    "    root = lxml.html.fromstring(response.text)\n",
    "    episodes = root.find_class(\"container mb-16\")\n",
    "    for episode in episodes:\n",
    "        for i,element in enumerate(episode.getchildren()):\n",
    "            rel_url = element.get(\"href\")\n",
    "            pod_url = make_link_absolute(rel_url, url)\n",
    "            for i,x in enumerate(element.getchildren()):\n",
    "                list = x.getchildren()\n",
    "                for i, text in enumerate(list):\n",
    "                    lst = text.getchildren()\n",
    "                    z = lst[1].getchildren()\n",
    "                    Episode_title = z[1].text_content()\n",
    "                    Podcast = \"Not Another Politics Podcast\"\n",
    "                    Description = z[2].text_content()\n",
    "                    tp = z[0].text_content()\n",
    "                    date = tp.split(\"|\")[1]\n",
    "                    formatted_date = datetime.datetime.strptime(date, \" %B %d, %Y\")\n",
    "                    value.append((Episode_title, Podcast, pod_url, date, Description))\n",
    "                    if formatted_date > end_date:\n",
    "                        break\n",
    "    for i in range(len(value)):\n",
    "        for j in range(len(value[i])):\n",
    "            sheet.cell(row = i + 1, column = j + 1, value = str(value[i][j]))\n",
    "    workbook.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5f4fb2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_news(range_num):\n",
    "    '''\n",
    "    scraping news from the harris news website\n",
    "    input:\n",
    "        range_num: the number of pages to scrape\n",
    "    '''\n",
    "    value = []\n",
    "    url_org = \"https://harris.uchicago.edu/news-events/news?page=\"\n",
    "    for i in range(range_num):\n",
    "        if i == 0:\n",
    "            url = url_org\n",
    "        if i != 0:\n",
    "            page = i \n",
    "            url = url_org + '?page=' + str(page) + '\"'\n",
    "        response = requests.get(url)\n",
    "        root = lxml.html.fromstring(response.text)\n",
    "        episodes = root.find_class(\"node--expanded\")\n",
    "        for episode in episodes:\n",
    "            text = episode.getchildren()\n",
    "            title = text[1].text_content().strip()\n",
    "            date = text[2].getchildren()[0].text_content().strip()\n",
    "            descrip = episode.find_class(\"node--expanded--summary\")\n",
    "            if len(descrip) != 0:\n",
    "                description = descrip[0].text_content().strip()\n",
    "            else:\n",
    "                description = \"\"\n",
    "            url = \"\"\n",
    "            value.append((title, url, date, description))\n",
    "    for i in range(len(value)):\n",
    "        for j in range(len(value[i])):\n",
    "            sheet.cell(row = i + 1, column = j + 1, value = str(value[i][j]))\n",
    "    workbook.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "30529eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_in_news(range_num):\n",
    "    '''\n",
    "    scraping news from the harris news website\n",
    "    input:\n",
    "        range_num: the number of pages to scrape\n",
    "    '''\n",
    "    value = []\n",
    "    url_org = \"https://harris.uchicago.edu/news-events/news/in-the-news\"\n",
    "    for i in range(range_num):\n",
    "        if i == 0:\n",
    "            url = url_org\n",
    "        if i != 0:\n",
    "            page = i\n",
    "            url = url_org + '?page=' + str(page) + '\"'\n",
    "        response = requests.get(url)\n",
    "        root = lxml.html.fromstring(response.text)\n",
    "        episodes = root.find_class(\"node--expanded\")\n",
    "        for episode in episodes:\n",
    "            text = episode.getchildren()\n",
    "            title = text[1].text_content().strip()\n",
    "            date = text[2].getchildren()[0].text_content().strip()\n",
    "            descrip = episode.find_class(\"node--expanded--summary\")\n",
    "            if len(descrip) != 0:\n",
    "                description = descrip[0].text_content().strip()\n",
    "            else:\n",
    "                description = \"\"\n",
    "            url = \"\"\n",
    "            value.append((title, url, date, description))\n",
    "    for i in range(len(value)):\n",
    "        for j in range(len(value[i])):\n",
    "            sheet.cell(row = i + 1, column = j + 1, value = str(value[i][j]))\n",
    "    workbook.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "49ed72af",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_in_news(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a1de2c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_public_money_pd(end_date = '2032-01-01'):\n",
    "    '''\n",
    "    scraping public money podcast after a certain date\n",
    "    input:\n",
    "        end_data(string): in format like \"2010-01-01\"\n",
    "        if no input: scrape the entire website\n",
    "    '''\n",
    "    value = []\n",
    "    import datetime\n",
    "    end_date = datetime.datetime.strptime(end_date, '%Y-%m-%d')\n",
    "    \n",
    "    url = 'https://munifinance.uchicago.edu/cmf-podcast/'\n",
    "    response = requests.get(url)\n",
    "    root = lxml.html.fromstring(response.text)\n",
    "    episodes = root.find_class(\"et_pb_code_inner\")[0].getchildren()\n",
    "    \n",
    "    for i, episode in enumerate(episodes):\n",
    "        iframe = episode.getchildren()[0]\n",
    "        sem = iframe.attrib['src']\n",
    "        response_sem = requests.get(sem)\n",
    "        sem_root = lxml.html.fromstring(response_sem.text)\n",
    "        title = sem_root.find_class('episode-title')[0].text_content()\n",
    "        podcast = 'Public Money Pod'\n",
    "        date = sem_root.find_class('episode-subtitle')[0].text_content()\n",
    "        cleaned_date = datetime.datetime.strptime(date.split('â€¢')[1].strip(),'%dth %B %Y')\n",
    "        value.append((title, podcast, sem, cleaned_date.strftime(\"%B %d, %Y\")))\n",
    "        if cleaned_date > end_date:\n",
    "            break\n",
    "            \n",
    "    for i in range(len(value)):\n",
    "        for j in range(len(value[i])):\n",
    "            sheet.cell(row = i + 1, column = j + 1, value = str(value[i][j]))\n",
    "    workbook.save(path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
